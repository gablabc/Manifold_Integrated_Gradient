{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Semantic_interpolation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfRIhRSok1uIFczgNeGJ3J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ptXrB598scG","executionInfo":{"status":"ok","timestamp":1619552965805,"user_tz":240,"elapsed":48668,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"4cbe6d55-2b34-4950-c8a8-461383a9ad7f"},"source":["# Allow Collab to connect to your Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Path to the data on Drive\n","root = \"/content/drive/MyDrive/PhD/Integrated_Gradient\"\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FXkJ5C3D81KU","executionInfo":{"status":"ok","timestamp":1619553043118,"user_tz":240,"elapsed":429,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from torchsummary import summary\n","import numpy as np\n","from math import floor\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWNbZKq284Yb","executionInfo":{"status":"ok","timestamp":1619552976433,"user_tz":240,"elapsed":600,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"b2c91464-68a0-49bd-a798-e25967db82f6"},"source":["# Checking if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    use_cuda = True\n","else:\n","    device = torch.device(\"cpu\")\n","    use_cuda = False\n","print(\"Found:\",torch.cuda.device_count(), device)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found: 1 cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IiLcF84M868W","executionInfo":{"status":"ok","timestamp":1619533853206,"user_tz":240,"elapsed":531,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["class VAE(nn.Module):\n","    def __init__(self, latent_dim, hidden_size):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.hidden_size = hidden_size\n","\n","        self.fc1 = nn.Linear(784, hidden_size)\n","        self.fc21 = nn.Linear(hidden_size, latent_dim)\n","        self.fc22 = nn.Linear(hidden_size, latent_dim)\n","        self.fc3 = nn.Linear(latent_dim, hidden_size)\n","        self.fc4 = nn.Linear(hidden_size, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x.view(-1, 784))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETdts_Ly4CHN","executionInfo":{"status":"ok","timestamp":1619553046862,"user_tz":240,"elapsed":424,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["class ConvVAE(nn.Module):\n","    def __init__(self):\n","        self.name = \"Conv_VAE\"\n","        super(ConvVAE, self).__init__()\n","        kernel_size      = 4   # (4 x 4)\n","        stride           = 2\n","        padding          = 1\n","        init_channels    = 16 # initial number of filters\n","        self.latent_dim  = 20 # latent dimension for sampling\n","\n","        # Encoder\n","        # Conv1\n","        self.enc1 = nn.Conv2d(in_channels = 1, out_channels = init_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((28 - kernel_size  + 2 * padding) / stride + 1)\n","\n","        # Conv2\n","        self.enc2 = nn.Conv2d(in_channels = init_channels, out_channels = init_channels * 2, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((height_width - kernel_size  + 2 * padding) / stride + 1)\n","\n","        # Conv3\n","        self.enc3 = nn.Conv2d(in_channels = init_channels * 2, out_channels = init_channels * 4, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((height_width - kernel_size  + 2 * padding) / stride + 1)\n","        \n","        # fully connected layers for learning representations\n","        hidden_size = height_width ** 2 * init_channels * 4\n","        self.fc_mu = nn.Linear(hidden_size, self.latent_dim)\n","        self.fc_log_var = nn.Linear(hidden_size, self.latent_dim)\n","        self.fc2 = nn.Linear(self.latent_dim, init_channels * 4)\n","        \n","        \n","\n","        # Decoder\n","        # ConvT  get width/height equal to kernel_size\n","        self.dec1 = nn.ConvTranspose2d(in_channels = init_channels * 4, out_channels = init_channels * 4, kernel_size = kernel_size, stride = 1, padding = 0)\n","        height_width = kernel_size\n","        \n","        # ConvT\n","        self.dec2 = nn.ConvTranspose2d(in_channels = init_channels * 4, out_channels = init_channels * 2, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = (height_width - 1) * stride - 2 * padding + kernel_size\n","        \n","        # ConvT\n","        self.dec3 = nn.ConvTranspose2d(in_channels = init_channels * 2, out_channels = init_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = (height_width - 1) * stride - 2 * padding + kernel_size\n","        \n","        # ConvT adjust the padding so that we end up at 28x28\n","        required_padding = int(((height_width - 1) * stride + kernel_size - 28) / 2) \n","        self.dec4 = nn.ConvTranspose2d(in_channels = init_channels, out_channels = 1, kernel_size = kernel_size, stride = stride, padding = required_padding)\n","\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var) # standard deviation\n","        eps = torch.randn_like(std) # `randn_like` as we need the same size\n","        return mu + eps * std # sampling\n","        \n","\n","    def encode(self, x):\n","        x = F.relu(self.enc1(x))\n","        x = F.relu(self.enc2(x))\n","        x = F.relu(self.enc3(x))\n","        hidden = x.view(x.shape[0], -1)\n","        return self.fc_mu(hidden), self.fc_log_var(hidden)\n","\n","\n","    def decode(self, z):\n","        z = self.fc2(z)\n","        z = z.view(-1, 64, 1, 1)\n","        x = F.relu(self.dec1(z))\n","        x = F.relu(self.dec2(x))\n","        x = F.relu(self.dec3(x))\n","        return torch.sigmoid(self.dec4(x))\n","\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipgtUI829B9J","executionInfo":{"status":"ok","timestamp":1619553054879,"user_tz":240,"elapsed":7364,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"2fe0b7d0-71a7-4e23-81ca-fbcb7228e4d9"},"source":["model = ConvVAE().to(device)\n","model.load_state_dict(torch.load(f\"{root}/models/{model.name}.pt\"))\n","summary(model, (1, 28, 28))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 14, 14]             272\n","            Conv2d-2             [-1, 32, 7, 7]           8,224\n","            Conv2d-3             [-1, 64, 3, 3]          32,832\n","            Linear-4                   [-1, 20]          11,540\n","            Linear-5                   [-1, 20]          11,540\n","            Linear-6                   [-1, 64]           1,344\n","   ConvTranspose2d-7             [-1, 64, 4, 4]          65,600\n","   ConvTranspose2d-8             [-1, 32, 8, 8]          32,800\n","   ConvTranspose2d-9           [-1, 16, 16, 16]           8,208\n","  ConvTranspose2d-10            [-1, 1, 28, 28]             257\n","================================================================\n","Total params: 172,617\n","Trainable params: 172,617\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.10\n","Params size (MB): 0.66\n","Estimated Total Size (MB): 0.76\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mw5_A-YwEumq","executionInfo":{"status":"ok","timestamp":1619553113297,"user_tz":240,"elapsed":1777,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"60c22316-54db-4e77-e8c0-b4626e21f8a1"},"source":["test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST(f\"{root}/dataset\", train=False, download=True, transform=transforms.ToTensor()),\n","        batch_size=64, shuffle=False)\n","image_batch, target_batch = next(iter(test_loader))\n","image_batch = image_batch.to(device)\n","target_batch = target_batch.to(device)\n","image_batch.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 1, 28, 28])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"8zCA3I6zGORe","executionInfo":{"status":"ok","timestamp":1619553130874,"user_tz":240,"elapsed":279,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["# Regular interpolation\n","n_samples = 15\n","alpha = torch.linspace(0, 1, n_samples)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOVgbCrPFVoB","executionInfo":{"status":"ok","timestamp":1619553134870,"user_tz":240,"elapsed":3369,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["for i in range(5):\n","    linear_interp = torch.cat([ a * image_batch[[i]] + (1 - a) * image_batch[[i + 1]] for a in alpha])\n","    save_image(linear_interp, f\"{root}/results/Interpolation/linear_interp_{target_batch[i]}_to_{target_batch[i + 1]}.png\", nrow=n_samples)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"WlCQAgORZQOn","executionInfo":{"status":"ok","timestamp":1619553192047,"user_tz":240,"elapsed":289,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["# spherical linear interpolation (slerp)\n","def slerp(z1, z2, alpha):\n","    omega = torch.acos(torch.sum(z1 / torch.norm(z1, dim = -1, keepdim = True) \\\n","                               * z2 / torch.norm(z2, dim = -1, keepdim = True), dim = -1, keepdims = True))\n","    so = torch.sin(omega)\n","    return torch.cat([torch.sin((1 - a) * omega) / so * z1 + torch.sin(a * omega) / so * z2 for a in alpha]), omega"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLb2USlkXP3g","executionInfo":{"status":"ok","timestamp":1619553220745,"user_tz":240,"elapsed":4343,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["# Compute latent repersentations\n","z, _ = model.encode(image_batch)\n","for i in range(5):\n","    linear_z_interp = torch.cat([ a * z[[i]] + (1 - a) * z[[i + 1]] for a in alpha])\n","    semantic_interp = model.decode(linear_z_interp)\n","    save_image(semantic_interp, f\"{root}/results/Interpolation/semantic_interp_{target_batch[i]}_to_{target_batch[i + 1]}.png\", nrow=n_samples)\n","\n","    slerp_z_interp, _ = slerp(z[[i + 1]], z[[i]], alpha)\n","    spherical_interp = model.decode(slerp_z_interp)\n","    save_image(spherical_interp, f\"{root}/results/Interpolation/spherical_interp_{target_batch[i]}_to_{target_batch[i + 1]}.png\", nrow=n_samples)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpuDtnXftRYg"},"source":[""],"execution_count":null,"outputs":[]}]}