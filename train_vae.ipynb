{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_vae.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNlJgkxxrA6wH2UOvufhUK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZW0vcfHliUk","executionInfo":{"status":"ok","timestamp":1619565486165,"user_tz":240,"elapsed":19677,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"1acf90f0-a0d2-430d-8b2d-337508f4b20e"},"source":["# Allow Collab to connect to your Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Path to the data on Drive\n","root = \"/content/drive/MyDrive/PhD/Integrated_Gradient\"\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_-jqmkWahnQO","executionInfo":{"status":"ok","timestamp":1619565490134,"user_tz":240,"elapsed":2731,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from torchsummary import summary\n","from math import floor\n","import matplotlib.pyplot as plt\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7Ftigy9iJVU","executionInfo":{"status":"ok","timestamp":1619565494285,"user_tz":240,"elapsed":3395,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"7b6edca3-fdf4-4b05-c706-8d26a2cd5a1a"},"source":["!pip install torchsummary"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fuGQIshiGrl","executionInfo":{"status":"ok","timestamp":1619565495575,"user_tz":240,"elapsed":478,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"68e5f976-7f0f-486c-9039-eb427fb625c8"},"source":["# Checking if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    use_cuda = True\n","else:\n","    device = torch.device(\"cpu\")\n","    use_cuda = False\n","print(\"Found:\",torch.cuda.device_count(), device)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Found: 1 cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wTv-c-Rfhxf6","executionInfo":{"status":"ok","timestamp":1619548778294,"user_tz":240,"elapsed":256,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["class VAE(nn.Module):\n","    def __init__(self, latent_dim, hidden_size):\n","        super().__init__()\n","        self.name = \"vanilla_VAE\"\n","        self.latent_dim = latent_dim\n","        self.hidden_size = hidden_size\n","\n","        self.fc1 = nn.Linear(784, hidden_size)\n","        self.fc21 = nn.Linear(hidden_size, latent_dim)\n","        self.fc22 = nn.Linear(hidden_size, latent_dim)\n","        self.fc3 = nn.Linear(latent_dim, hidden_size)\n","        self.fc4 = nn.Linear(hidden_size, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3)).view(-1, 1, 28, 28)\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x.view(-1, 784))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRB7-Ljd3P-h","executionInfo":{"status":"ok","timestamp":1619565511900,"user_tz":240,"elapsed":517,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["class ConvVAE(nn.Module):\n","    def __init__(self):\n","        self.name = \"Conv_VAE\"\n","        super(ConvVAE, self).__init__()\n","        kernel_size      = 4   # (4 x 4)\n","        stride           = 2\n","        padding          = 1\n","        init_channels    = 16 # initial number of filters\n","        self.latent_dim  = 20 # latent dimension for sampling\n","\n","        # Encoder\n","        # Conv1\n","        self.enc1 = nn.Conv2d(in_channels = 1, out_channels = init_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((28 - kernel_size  + 2 * padding) / stride + 1)\n","\n","        # Conv2\n","        self.enc2 = nn.Conv2d(in_channels = init_channels, out_channels = init_channels * 2, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((height_width - kernel_size  + 2 * padding) / stride + 1)\n","\n","        # Conv3\n","        self.enc3 = nn.Conv2d(in_channels = init_channels * 2, out_channels = init_channels * 4, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = floor((height_width - kernel_size  + 2 * padding) / stride + 1)\n","        \n","        # fully connected layers for learning representations\n","        hidden_size = height_width ** 2 * init_channels * 4\n","        self.fc_mu = nn.Linear(hidden_size, self.latent_dim)\n","        self.fc_log_var = nn.Linear(hidden_size, self.latent_dim)\n","        self.fc2 = nn.Linear(self.latent_dim, init_channels * 4)\n","        \n","        \n","\n","        # Decoder\n","        # ConvT  get width/height equal to kernel_size\n","        self.dec1 = nn.ConvTranspose2d(in_channels = init_channels * 4, out_channels = init_channels * 4, kernel_size = kernel_size, stride = 1, padding = 0)\n","        height_width = kernel_size\n","        \n","        # ConvT\n","        self.dec2 = nn.ConvTranspose2d(in_channels = init_channels * 4, out_channels = init_channels * 2, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = (height_width - 1) * stride - 2 * padding + kernel_size\n","        \n","        # ConvT\n","        self.dec3 = nn.ConvTranspose2d(in_channels = init_channels * 2, out_channels = init_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n","        height_width = (height_width - 1) * stride - 2 * padding + kernel_size\n","        \n","        # ConvT adjust the padding so that we end up at 28x28\n","        required_padding = int(((height_width - 1) * stride + kernel_size - 28) / 2) \n","        self.dec4 = nn.ConvTranspose2d(in_channels = init_channels, out_channels = 1, kernel_size = kernel_size, stride = stride, padding = required_padding)\n","\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5 * log_var) # standard deviation\n","        eps = torch.randn_like(std) # `randn_like` as we need the same size\n","        return mu + eps * std # sampling\n","        \n","\n","    def encode(self, x):\n","        x = F.relu(self.enc1(x))\n","        x = F.relu(self.enc2(x))\n","        x = F.relu(self.enc3(x))\n","        hidden = x.view(x.shape[0], -1)\n","        return self.fc_mu(hidden), self.fc_log_var(hidden)\n","\n","\n","    def decode(self, z):\n","        z = self.fc2(z)\n","        z = z.view(-1, 64, 1, 1)\n","        x = F.relu(self.dec1(z))\n","        x = F.relu(self.dec2(x))\n","        x = F.relu(self.dec3(x))\n","        return torch.sigmoid(self.dec4(x))\n","\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhFKLwPVPump","executionInfo":{"status":"ok","timestamp":1619565559341,"user_tz":240,"elapsed":6452,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"56ee79b3-ce9b-4f27-8b8c-b955c416e37b"},"source":["# to debug\n","#model = VAE(20, 600).to(device)\n","model = ConvVAE().to(device)\n","print(model)\n","summary(model, (1, 28, 28))\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["ConvVAE(\n","  (enc1): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (enc2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (enc3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (fc_mu): Linear(in_features=576, out_features=20, bias=True)\n","  (fc_log_var): Linear(in_features=576, out_features=20, bias=True)\n","  (fc2): Linear(in_features=20, out_features=64, bias=True)\n","  (dec1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(1, 1))\n","  (dec2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (dec3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (dec4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3))\n",")\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 14, 14]             272\n","            Conv2d-2             [-1, 32, 7, 7]           8,224\n","            Conv2d-3             [-1, 64, 3, 3]          32,832\n","            Linear-4                   [-1, 20]          11,540\n","            Linear-5                   [-1, 20]          11,540\n","            Linear-6                   [-1, 64]           1,344\n","   ConvTranspose2d-7             [-1, 64, 4, 4]          65,600\n","   ConvTranspose2d-8             [-1, 32, 8, 8]          32,800\n","   ConvTranspose2d-9           [-1, 16, 16, 16]           8,208\n","  ConvTranspose2d-10            [-1, 1, 28, 28]             257\n","================================================================\n","Total params: 172,617\n","Trainable params: 172,617\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.10\n","Params size (MB): 0.66\n","Estimated Total Size (MB): 0.76\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iL9Y06AJ32LH","executionInfo":{"status":"ok","timestamp":1619550716699,"user_tz":240,"elapsed":266,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","\n","    # see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    # https://arxiv.org/abs/1312.6114\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","\n","    return BCE + KLD"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZAb4sOCaqhz","executionInfo":{"status":"ok","timestamp":1619552119531,"user_tz":240,"elapsed":259,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["class Logger:\n","    \" Simple logger to track progress during training\"\n","    def __init__(self):\n","        self.losses_train = []\n","        self.losses_test = []\n","\n","    def log(self, loss_train = 0, loss_test = 0):\n","        self.losses_train.append(loss_train)\n","        self.losses_test.append(loss_test)\n","\n","def plot_learning_curve(train_curve, test_curve, ticks = 2):\n","    plt.figure()\n","    plt.plot(train_curve, label=\"Train\")\n","    plt.plot(test_curve, label=\"Test\")\n","    plt.legend(loc = 'best')\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"ELBO\")\n","    plt.grid(True, alpha = 0.5)\n"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ohSs_rhh0mK","executionInfo":{"status":"ok","timestamp":1619550764695,"user_tz":240,"elapsed":237,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["def train(epoch, model, train_loader, optimizer, device, log_interval = 50):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)\n","        loss = loss_function(recon_batch, data, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.item() / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))\n","    \n","    return train_loss / len(train_loader.dataset)\n"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPgJnQn8h3ki","executionInfo":{"status":"ok","timestamp":1619550765282,"user_tz":240,"elapsed":163,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["def test(epoch, model, test_loader, device, batch_size = 128):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for i, (data, _) in enumerate(test_loader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n","            if epoch % 4 == 0 and i == 0:\n","                n = min(data.size(0), 8)\n","                comparison = torch.cat([data[:n],\n","                                        recon_batch[:n]])\n","                save_image(comparison.cpu(),\n","                         root + '/results/VAE/recons/reconstruction_' + str(epoch) + '.png', nrow=n)\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))\n","\n","    return test_loss\n"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOsCjCXsh3pQ","executionInfo":{"status":"ok","timestamp":1619550767024,"user_tz":240,"elapsed":368,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}}},"source":["def main(model, batch_size, epochs, lr = 5e-4):\n","    #torch.manual_seed(seed)\n","    kwargs = {'num_workers': 1, 'pin_memory': True}\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST(f\"{root}/dataset/\", train=True, download=True,\n","                    transform=transforms.ToTensor()),\n","        batch_size=batch_size, shuffle=True, **kwargs)\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST(f\"{root}/dataset/\", train=False, transform=transforms.ToTensor()),\n","        batch_size=batch_size, shuffle=True, **kwargs)\n","\n","    optimizer = optim.Adam(model.parameters(), lr = lr)\n","    logger = Logger()\n","\n","    for epoch in range(1, epochs + 1):\n","        \n","        train_loss = train(epoch, model, train_loader, optimizer, device)\n","        test_loss = test(epoch, model, test_loader, device, batch_size)\n","        logger.log(train_loss, test_loss)\n","\n","        if epoch % 4 == 0:\n","            with torch.no_grad():\n","                sample = torch.randn(64, model.latent_dim).to(device)\n","                sample = model.decode(sample).cpu()\n","                save_image(sample,\n","                        root + '/results/VAE/samples/sample_' + str(epoch) + '.png')\n","    torch.save(model.state_dict(), f\"{root}/models/{model.name}.pt\")\n","\n","    return logger\n","    "],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZq5VdAVkSQh","executionInfo":{"status":"ok","timestamp":1619551528244,"user_tz":240,"elapsed":593681,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"4c31e1dd-ef8b-429e-f5bc-fefffb8b2d34"},"source":["#model = VAE(20, 600).to(device)\n","model = ConvVAE().to(device)\n","summary(model, (1, 28, 28))\n","logger = main(model, batch_size = 128, epochs = 80, lr = 1e-3)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 14, 14]             272\n","            Conv2d-2             [-1, 32, 7, 7]           8,224\n","            Conv2d-3             [-1, 64, 3, 3]          32,832\n","            Linear-4                   [-1, 20]          11,540\n","            Linear-5                   [-1, 20]          11,540\n","            Linear-6                   [-1, 64]           1,344\n","   ConvTranspose2d-7             [-1, 64, 4, 4]          65,600\n","   ConvTranspose2d-8             [-1, 32, 8, 8]          32,800\n","   ConvTranspose2d-9           [-1, 16, 16, 16]           8,208\n","  ConvTranspose2d-10            [-1, 1, 28, 28]             257\n","================================================================\n","Total params: 172,617\n","Trainable params: 172,617\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.10\n","Params size (MB): 0.66\n","Estimated Total Size (MB): 0.76\n","----------------------------------------------------------------\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.007874\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 221.065735\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 208.200058\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 194.073776\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 172.368027\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 154.015839\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 151.363525\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 139.774460\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 125.918640\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 127.086838\n","====> Epoch: 1 Average loss: 183.9469\n","====> Test set loss: 124.6944\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 126.450409\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 122.639114\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 123.913383\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 115.272873\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 115.020172\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 114.435349\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 113.504547\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 111.563858\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 108.806190\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 115.521378\n","====> Epoch: 2 Average loss: 115.7982\n","====> Test set loss: 109.5328\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 107.093674\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 111.089462\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 106.666748\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 105.476143\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 107.984665\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 107.625656\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 107.813766\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 104.185722\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 112.849380\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 100.374481\n","====> Epoch: 3 Average loss: 108.3942\n","====> Test set loss: 106.5669\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 107.878036\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 101.388245\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 105.705208\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 107.478897\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 106.306091\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 109.379448\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.996841\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 102.607025\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 104.792923\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 106.704819\n","====> Epoch: 4 Average loss: 105.6260\n","====> Test set loss: 103.8491\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 103.633751\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 104.113754\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 105.484764\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 102.776382\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 99.952980\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 103.712097\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 99.428215\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 105.803955\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 102.187775\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 104.662003\n","====> Epoch: 5 Average loss: 103.9923\n","====> Test set loss: 102.6848\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 101.050842\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 105.712433\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 106.330627\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 103.396431\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 102.982231\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 99.865112\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 103.653236\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 105.835281\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 104.096146\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 102.642334\n","====> Epoch: 6 Average loss: 102.9154\n","====> Test set loss: 101.9139\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 107.735764\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 102.893112\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 102.869537\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 105.156342\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 104.526253\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 103.010902\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 103.199234\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 106.241661\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 100.031311\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 101.764465\n","====> Epoch: 7 Average loss: 102.0987\n","====> Test set loss: 101.2907\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 100.891571\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 101.201599\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 101.998825\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 98.761879\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 100.265633\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 101.874466\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 99.106682\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 104.438583\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 100.000244\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 102.008751\n","====> Epoch: 8 Average loss: 101.4954\n","====> Test set loss: 101.1421\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 102.308945\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 99.196365\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 101.747864\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 99.721756\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 99.171997\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 101.463165\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 99.100891\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 97.923920\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 102.123749\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 99.183685\n","====> Epoch: 9 Average loss: 100.9758\n","====> Test set loss: 100.2423\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.875664\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 103.626541\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 101.331818\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 97.119576\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 101.900009\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 103.848289\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 101.457809\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 101.007126\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 99.782501\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 97.799454\n","====> Epoch: 10 Average loss: 100.5843\n","====> Test set loss: 100.0132\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 102.131226\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 100.129074\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 105.192612\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 101.704964\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 98.926308\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 96.826889\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 97.330482\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 104.990723\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 103.250763\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 95.274445\n","====> Epoch: 11 Average loss: 100.2148\n","====> Test set loss: 99.6228\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 99.660568\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 95.599358\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 100.064331\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 100.576408\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 102.104759\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 98.953201\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 98.993759\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 102.714386\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 99.602264\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 102.768623\n","====> Epoch: 12 Average loss: 99.8652\n","====> Test set loss: 99.6786\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 95.767441\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 98.289078\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 101.121597\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 99.025833\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 100.342789\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 97.877991\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 100.209183\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 101.829468\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 104.386047\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 97.945793\n","====> Epoch: 13 Average loss: 99.6742\n","====> Test set loss: 99.5474\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 98.368591\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 98.926003\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 96.910301\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 98.022270\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 102.338074\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 96.362915\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 99.235794\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 102.554253\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 97.813637\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 99.756752\n","====> Epoch: 14 Average loss: 99.4304\n","====> Test set loss: 98.8265\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 102.170258\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 100.411636\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 98.825981\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 102.766411\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 103.469597\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 95.204231\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 101.982864\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 100.422623\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 100.868210\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 98.141739\n","====> Epoch: 15 Average loss: 99.1467\n","====> Test set loss: 98.9233\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 96.490936\n","Train Epoch: 16 [6400/60000 (11%)]\tLoss: 95.451263\n","Train Epoch: 16 [12800/60000 (21%)]\tLoss: 97.521576\n","Train Epoch: 16 [19200/60000 (32%)]\tLoss: 97.913010\n","Train Epoch: 16 [25600/60000 (43%)]\tLoss: 96.131287\n","Train Epoch: 16 [32000/60000 (53%)]\tLoss: 97.221603\n","Train Epoch: 16 [38400/60000 (64%)]\tLoss: 100.562904\n","Train Epoch: 16 [44800/60000 (75%)]\tLoss: 98.229996\n","Train Epoch: 16 [51200/60000 (85%)]\tLoss: 96.792534\n","Train Epoch: 16 [57600/60000 (96%)]\tLoss: 98.527771\n","====> Epoch: 16 Average loss: 99.0134\n","====> Test set loss: 98.8315\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 96.272789\n","Train Epoch: 17 [6400/60000 (11%)]\tLoss: 100.302612\n","Train Epoch: 17 [12800/60000 (21%)]\tLoss: 98.539017\n","Train Epoch: 17 [19200/60000 (32%)]\tLoss: 97.202843\n","Train Epoch: 17 [25600/60000 (43%)]\tLoss: 97.934143\n","Train Epoch: 17 [32000/60000 (53%)]\tLoss: 99.216522\n","Train Epoch: 17 [38400/60000 (64%)]\tLoss: 97.253586\n","Train Epoch: 17 [44800/60000 (75%)]\tLoss: 99.693680\n","Train Epoch: 17 [51200/60000 (85%)]\tLoss: 102.615021\n","Train Epoch: 17 [57600/60000 (96%)]\tLoss: 96.874687\n","====> Epoch: 17 Average loss: 98.7437\n","====> Test set loss: 98.3071\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 97.904518\n","Train Epoch: 18 [6400/60000 (11%)]\tLoss: 101.148438\n","Train Epoch: 18 [12800/60000 (21%)]\tLoss: 100.540459\n","Train Epoch: 18 [19200/60000 (32%)]\tLoss: 97.217720\n","Train Epoch: 18 [25600/60000 (43%)]\tLoss: 102.109016\n","Train Epoch: 18 [32000/60000 (53%)]\tLoss: 98.078117\n","Train Epoch: 18 [38400/60000 (64%)]\tLoss: 96.711838\n","Train Epoch: 18 [44800/60000 (75%)]\tLoss: 98.446930\n","Train Epoch: 18 [51200/60000 (85%)]\tLoss: 99.427765\n","Train Epoch: 18 [57600/60000 (96%)]\tLoss: 100.995926\n","====> Epoch: 18 Average loss: 98.6042\n","====> Test set loss: 98.5910\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 93.966866\n","Train Epoch: 19 [6400/60000 (11%)]\tLoss: 99.011200\n","Train Epoch: 19 [12800/60000 (21%)]\tLoss: 100.115051\n","Train Epoch: 19 [19200/60000 (32%)]\tLoss: 96.078049\n","Train Epoch: 19 [25600/60000 (43%)]\tLoss: 99.707489\n","Train Epoch: 19 [32000/60000 (53%)]\tLoss: 99.004509\n","Train Epoch: 19 [38400/60000 (64%)]\tLoss: 97.162827\n","Train Epoch: 19 [44800/60000 (75%)]\tLoss: 97.333206\n","Train Epoch: 19 [51200/60000 (85%)]\tLoss: 99.445984\n","Train Epoch: 19 [57600/60000 (96%)]\tLoss: 100.823456\n","====> Epoch: 19 Average loss: 98.4174\n","====> Test set loss: 98.2367\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 97.030258\n","Train Epoch: 20 [6400/60000 (11%)]\tLoss: 98.734314\n","Train Epoch: 20 [12800/60000 (21%)]\tLoss: 96.773094\n","Train Epoch: 20 [19200/60000 (32%)]\tLoss: 97.512390\n","Train Epoch: 20 [25600/60000 (43%)]\tLoss: 96.555267\n","Train Epoch: 20 [32000/60000 (53%)]\tLoss: 99.916054\n","Train Epoch: 20 [38400/60000 (64%)]\tLoss: 94.828209\n","Train Epoch: 20 [44800/60000 (75%)]\tLoss: 96.066086\n","Train Epoch: 20 [51200/60000 (85%)]\tLoss: 99.932053\n","Train Epoch: 20 [57600/60000 (96%)]\tLoss: 99.004822\n","====> Epoch: 20 Average loss: 98.3592\n","====> Test set loss: 97.7504\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 101.059731\n","Train Epoch: 21 [6400/60000 (11%)]\tLoss: 95.124863\n","Train Epoch: 21 [12800/60000 (21%)]\tLoss: 94.607025\n","Train Epoch: 21 [19200/60000 (32%)]\tLoss: 95.656914\n","Train Epoch: 21 [25600/60000 (43%)]\tLoss: 95.494698\n","Train Epoch: 21 [32000/60000 (53%)]\tLoss: 99.171036\n","Train Epoch: 21 [38400/60000 (64%)]\tLoss: 97.824837\n","Train Epoch: 21 [44800/60000 (75%)]\tLoss: 97.228233\n","Train Epoch: 21 [51200/60000 (85%)]\tLoss: 94.310638\n","Train Epoch: 21 [57600/60000 (96%)]\tLoss: 97.433334\n","====> Epoch: 21 Average loss: 98.1765\n","====> Test set loss: 97.9965\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 101.209572\n","Train Epoch: 22 [6400/60000 (11%)]\tLoss: 96.664345\n","Train Epoch: 22 [12800/60000 (21%)]\tLoss: 98.897385\n","Train Epoch: 22 [19200/60000 (32%)]\tLoss: 99.889893\n","Train Epoch: 22 [25600/60000 (43%)]\tLoss: 99.119247\n","Train Epoch: 22 [32000/60000 (53%)]\tLoss: 99.668861\n","Train Epoch: 22 [38400/60000 (64%)]\tLoss: 96.875748\n","Train Epoch: 22 [44800/60000 (75%)]\tLoss: 95.872513\n","Train Epoch: 22 [51200/60000 (85%)]\tLoss: 100.252274\n","Train Epoch: 22 [57600/60000 (96%)]\tLoss: 96.531097\n","====> Epoch: 22 Average loss: 98.0123\n","====> Test set loss: 98.0538\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 97.743492\n","Train Epoch: 23 [6400/60000 (11%)]\tLoss: 99.168793\n","Train Epoch: 23 [12800/60000 (21%)]\tLoss: 99.844284\n","Train Epoch: 23 [19200/60000 (32%)]\tLoss: 96.376129\n","Train Epoch: 23 [25600/60000 (43%)]\tLoss: 97.382706\n","Train Epoch: 23 [32000/60000 (53%)]\tLoss: 103.006699\n","Train Epoch: 23 [38400/60000 (64%)]\tLoss: 99.949188\n","Train Epoch: 23 [44800/60000 (75%)]\tLoss: 99.030998\n","Train Epoch: 23 [51200/60000 (85%)]\tLoss: 99.149918\n","Train Epoch: 23 [57600/60000 (96%)]\tLoss: 91.558754\n","====> Epoch: 23 Average loss: 97.9522\n","====> Test set loss: 97.5347\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 97.804337\n","Train Epoch: 24 [6400/60000 (11%)]\tLoss: 93.447487\n","Train Epoch: 24 [12800/60000 (21%)]\tLoss: 100.322594\n","Train Epoch: 24 [19200/60000 (32%)]\tLoss: 99.795654\n","Train Epoch: 24 [25600/60000 (43%)]\tLoss: 93.864555\n","Train Epoch: 24 [32000/60000 (53%)]\tLoss: 92.418991\n","Train Epoch: 24 [38400/60000 (64%)]\tLoss: 96.407021\n","Train Epoch: 24 [44800/60000 (75%)]\tLoss: 95.002892\n","Train Epoch: 24 [51200/60000 (85%)]\tLoss: 102.421379\n","Train Epoch: 24 [57600/60000 (96%)]\tLoss: 96.154655\n","====> Epoch: 24 Average loss: 97.8138\n","====> Test set loss: 98.0950\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 95.275322\n","Train Epoch: 25 [6400/60000 (11%)]\tLoss: 95.515701\n","Train Epoch: 25 [12800/60000 (21%)]\tLoss: 99.818611\n","Train Epoch: 25 [19200/60000 (32%)]\tLoss: 98.907501\n","Train Epoch: 25 [25600/60000 (43%)]\tLoss: 98.328224\n","Train Epoch: 25 [32000/60000 (53%)]\tLoss: 97.743576\n","Train Epoch: 25 [38400/60000 (64%)]\tLoss: 100.037079\n","Train Epoch: 25 [44800/60000 (75%)]\tLoss: 99.338135\n","Train Epoch: 25 [51200/60000 (85%)]\tLoss: 93.576530\n","Train Epoch: 25 [57600/60000 (96%)]\tLoss: 99.444733\n","====> Epoch: 25 Average loss: 97.7264\n","====> Test set loss: 97.8913\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 100.381142\n","Train Epoch: 26 [6400/60000 (11%)]\tLoss: 99.907104\n","Train Epoch: 26 [12800/60000 (21%)]\tLoss: 96.152176\n","Train Epoch: 26 [19200/60000 (32%)]\tLoss: 97.923538\n","Train Epoch: 26 [25600/60000 (43%)]\tLoss: 97.533478\n","Train Epoch: 26 [32000/60000 (53%)]\tLoss: 100.277328\n","Train Epoch: 26 [38400/60000 (64%)]\tLoss: 95.758385\n","Train Epoch: 26 [44800/60000 (75%)]\tLoss: 95.746353\n","Train Epoch: 26 [51200/60000 (85%)]\tLoss: 91.643890\n","Train Epoch: 26 [57600/60000 (96%)]\tLoss: 99.283142\n","====> Epoch: 26 Average loss: 97.6067\n","====> Test set loss: 97.5619\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 97.466499\n","Train Epoch: 27 [6400/60000 (11%)]\tLoss: 97.887634\n","Train Epoch: 27 [12800/60000 (21%)]\tLoss: 98.226578\n","Train Epoch: 27 [19200/60000 (32%)]\tLoss: 97.727158\n","Train Epoch: 27 [25600/60000 (43%)]\tLoss: 96.213074\n","Train Epoch: 27 [32000/60000 (53%)]\tLoss: 95.562782\n","Train Epoch: 27 [38400/60000 (64%)]\tLoss: 94.193085\n","Train Epoch: 27 [44800/60000 (75%)]\tLoss: 101.285156\n","Train Epoch: 27 [51200/60000 (85%)]\tLoss: 96.137207\n","Train Epoch: 27 [57600/60000 (96%)]\tLoss: 98.705414\n","====> Epoch: 27 Average loss: 97.5308\n","====> Test set loss: 97.2100\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 98.820526\n","Train Epoch: 28 [6400/60000 (11%)]\tLoss: 98.845787\n","Train Epoch: 28 [12800/60000 (21%)]\tLoss: 97.513145\n","Train Epoch: 28 [19200/60000 (32%)]\tLoss: 96.054459\n","Train Epoch: 28 [25600/60000 (43%)]\tLoss: 98.142159\n","Train Epoch: 28 [32000/60000 (53%)]\tLoss: 98.364433\n","Train Epoch: 28 [38400/60000 (64%)]\tLoss: 98.238800\n","Train Epoch: 28 [44800/60000 (75%)]\tLoss: 98.953529\n","Train Epoch: 28 [51200/60000 (85%)]\tLoss: 97.405518\n","Train Epoch: 28 [57600/60000 (96%)]\tLoss: 98.974525\n","====> Epoch: 28 Average loss: 97.4172\n","====> Test set loss: 97.3120\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 97.210556\n","Train Epoch: 29 [6400/60000 (11%)]\tLoss: 96.701111\n","Train Epoch: 29 [12800/60000 (21%)]\tLoss: 93.185349\n","Train Epoch: 29 [19200/60000 (32%)]\tLoss: 96.386772\n","Train Epoch: 29 [25600/60000 (43%)]\tLoss: 95.318352\n","Train Epoch: 29 [32000/60000 (53%)]\tLoss: 99.125534\n","Train Epoch: 29 [38400/60000 (64%)]\tLoss: 94.148323\n","Train Epoch: 29 [44800/60000 (75%)]\tLoss: 100.535065\n","Train Epoch: 29 [51200/60000 (85%)]\tLoss: 95.905228\n","Train Epoch: 29 [57600/60000 (96%)]\tLoss: 95.209717\n","====> Epoch: 29 Average loss: 97.3710\n","====> Test set loss: 97.2258\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 97.811981\n","Train Epoch: 30 [6400/60000 (11%)]\tLoss: 99.574089\n","Train Epoch: 30 [12800/60000 (21%)]\tLoss: 98.112511\n","Train Epoch: 30 [19200/60000 (32%)]\tLoss: 97.096542\n","Train Epoch: 30 [25600/60000 (43%)]\tLoss: 92.507790\n","Train Epoch: 30 [32000/60000 (53%)]\tLoss: 98.388260\n","Train Epoch: 30 [38400/60000 (64%)]\tLoss: 98.234985\n","Train Epoch: 30 [44800/60000 (75%)]\tLoss: 100.672760\n","Train Epoch: 30 [51200/60000 (85%)]\tLoss: 98.270828\n","Train Epoch: 30 [57600/60000 (96%)]\tLoss: 98.517960\n","====> Epoch: 30 Average loss: 97.2621\n","====> Test set loss: 97.2694\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 94.923935\n","Train Epoch: 31 [6400/60000 (11%)]\tLoss: 95.638947\n","Train Epoch: 31 [12800/60000 (21%)]\tLoss: 93.688934\n","Train Epoch: 31 [19200/60000 (32%)]\tLoss: 98.444351\n","Train Epoch: 31 [25600/60000 (43%)]\tLoss: 96.767441\n","Train Epoch: 31 [32000/60000 (53%)]\tLoss: 96.725159\n","Train Epoch: 31 [38400/60000 (64%)]\tLoss: 98.609360\n","Train Epoch: 31 [44800/60000 (75%)]\tLoss: 98.969109\n","Train Epoch: 31 [51200/60000 (85%)]\tLoss: 97.225365\n","Train Epoch: 31 [57600/60000 (96%)]\tLoss: 98.040512\n","====> Epoch: 31 Average loss: 97.2214\n","====> Test set loss: 97.2137\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 93.909462\n","Train Epoch: 32 [6400/60000 (11%)]\tLoss: 97.172714\n","Train Epoch: 32 [12800/60000 (21%)]\tLoss: 97.218231\n","Train Epoch: 32 [19200/60000 (32%)]\tLoss: 95.741814\n","Train Epoch: 32 [25600/60000 (43%)]\tLoss: 97.494568\n","Train Epoch: 32 [32000/60000 (53%)]\tLoss: 98.166672\n","Train Epoch: 32 [38400/60000 (64%)]\tLoss: 99.126511\n","Train Epoch: 32 [44800/60000 (75%)]\tLoss: 97.831078\n","Train Epoch: 32 [51200/60000 (85%)]\tLoss: 98.332474\n","Train Epoch: 32 [57600/60000 (96%)]\tLoss: 100.431305\n","====> Epoch: 32 Average loss: 97.1701\n","====> Test set loss: 97.0038\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 97.608597\n","Train Epoch: 33 [6400/60000 (11%)]\tLoss: 97.505974\n","Train Epoch: 33 [12800/60000 (21%)]\tLoss: 95.740425\n","Train Epoch: 33 [19200/60000 (32%)]\tLoss: 96.838730\n","Train Epoch: 33 [25600/60000 (43%)]\tLoss: 97.828369\n","Train Epoch: 33 [32000/60000 (53%)]\tLoss: 98.234917\n","Train Epoch: 33 [38400/60000 (64%)]\tLoss: 98.313919\n","Train Epoch: 33 [44800/60000 (75%)]\tLoss: 95.904762\n","Train Epoch: 33 [51200/60000 (85%)]\tLoss: 98.518509\n","Train Epoch: 33 [57600/60000 (96%)]\tLoss: 99.343742\n","====> Epoch: 33 Average loss: 97.0476\n","====> Test set loss: 97.0869\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 96.939056\n","Train Epoch: 34 [6400/60000 (11%)]\tLoss: 94.564735\n","Train Epoch: 34 [12800/60000 (21%)]\tLoss: 97.659973\n","Train Epoch: 34 [19200/60000 (32%)]\tLoss: 100.428337\n","Train Epoch: 34 [25600/60000 (43%)]\tLoss: 99.445297\n","Train Epoch: 34 [32000/60000 (53%)]\tLoss: 97.135262\n","Train Epoch: 34 [38400/60000 (64%)]\tLoss: 95.636017\n","Train Epoch: 34 [44800/60000 (75%)]\tLoss: 95.555626\n","Train Epoch: 34 [51200/60000 (85%)]\tLoss: 97.929947\n","Train Epoch: 34 [57600/60000 (96%)]\tLoss: 99.847290\n","====> Epoch: 34 Average loss: 97.0627\n","====> Test set loss: 97.4228\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 95.689568\n","Train Epoch: 35 [6400/60000 (11%)]\tLoss: 96.633301\n","Train Epoch: 35 [12800/60000 (21%)]\tLoss: 95.623772\n","Train Epoch: 35 [19200/60000 (32%)]\tLoss: 96.553467\n","Train Epoch: 35 [25600/60000 (43%)]\tLoss: 97.818619\n","Train Epoch: 35 [32000/60000 (53%)]\tLoss: 102.182808\n","Train Epoch: 35 [38400/60000 (64%)]\tLoss: 100.197014\n","Train Epoch: 35 [44800/60000 (75%)]\tLoss: 99.581497\n","Train Epoch: 35 [51200/60000 (85%)]\tLoss: 96.326424\n","Train Epoch: 35 [57600/60000 (96%)]\tLoss: 98.941200\n","====> Epoch: 35 Average loss: 96.9571\n","====> Test set loss: 97.1047\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 98.830627\n","Train Epoch: 36 [6400/60000 (11%)]\tLoss: 98.654327\n","Train Epoch: 36 [12800/60000 (21%)]\tLoss: 98.499817\n","Train Epoch: 36 [19200/60000 (32%)]\tLoss: 97.813599\n","Train Epoch: 36 [25600/60000 (43%)]\tLoss: 96.330795\n","Train Epoch: 36 [32000/60000 (53%)]\tLoss: 96.544571\n","Train Epoch: 36 [38400/60000 (64%)]\tLoss: 97.913162\n","Train Epoch: 36 [44800/60000 (75%)]\tLoss: 101.593979\n","Train Epoch: 36 [51200/60000 (85%)]\tLoss: 93.862091\n","Train Epoch: 36 [57600/60000 (96%)]\tLoss: 94.464714\n","====> Epoch: 36 Average loss: 96.8999\n","====> Test set loss: 96.7735\n","Train Epoch: 37 [0/60000 (0%)]\tLoss: 99.423225\n","Train Epoch: 37 [6400/60000 (11%)]\tLoss: 99.178093\n","Train Epoch: 37 [12800/60000 (21%)]\tLoss: 95.077446\n","Train Epoch: 37 [19200/60000 (32%)]\tLoss: 97.040718\n","Train Epoch: 37 [25600/60000 (43%)]\tLoss: 95.452469\n","Train Epoch: 37 [32000/60000 (53%)]\tLoss: 94.155869\n","Train Epoch: 37 [38400/60000 (64%)]\tLoss: 98.055908\n","Train Epoch: 37 [44800/60000 (75%)]\tLoss: 98.826393\n","Train Epoch: 37 [51200/60000 (85%)]\tLoss: 93.099319\n","Train Epoch: 37 [57600/60000 (96%)]\tLoss: 95.466225\n","====> Epoch: 37 Average loss: 96.8329\n","====> Test set loss: 96.7952\n","Train Epoch: 38 [0/60000 (0%)]\tLoss: 97.623451\n","Train Epoch: 38 [6400/60000 (11%)]\tLoss: 95.115601\n","Train Epoch: 38 [12800/60000 (21%)]\tLoss: 95.018265\n","Train Epoch: 38 [19200/60000 (32%)]\tLoss: 98.986115\n","Train Epoch: 38 [25600/60000 (43%)]\tLoss: 98.859924\n","Train Epoch: 38 [32000/60000 (53%)]\tLoss: 96.125900\n","Train Epoch: 38 [38400/60000 (64%)]\tLoss: 96.182922\n","Train Epoch: 38 [44800/60000 (75%)]\tLoss: 94.082413\n","Train Epoch: 38 [51200/60000 (85%)]\tLoss: 100.025429\n","Train Epoch: 38 [57600/60000 (96%)]\tLoss: 94.343552\n","====> Epoch: 38 Average loss: 96.8241\n","====> Test set loss: 96.8585\n","Train Epoch: 39 [0/60000 (0%)]\tLoss: 95.156471\n","Train Epoch: 39 [6400/60000 (11%)]\tLoss: 98.781166\n","Train Epoch: 39 [12800/60000 (21%)]\tLoss: 99.509689\n","Train Epoch: 39 [19200/60000 (32%)]\tLoss: 95.440338\n","Train Epoch: 39 [25600/60000 (43%)]\tLoss: 100.079315\n","Train Epoch: 39 [32000/60000 (53%)]\tLoss: 91.877747\n","Train Epoch: 39 [38400/60000 (64%)]\tLoss: 97.325882\n","Train Epoch: 39 [44800/60000 (75%)]\tLoss: 96.995163\n","Train Epoch: 39 [51200/60000 (85%)]\tLoss: 99.401398\n","Train Epoch: 39 [57600/60000 (96%)]\tLoss: 98.732529\n","====> Epoch: 39 Average loss: 96.7203\n","====> Test set loss: 97.1172\n","Train Epoch: 40 [0/60000 (0%)]\tLoss: 96.474030\n","Train Epoch: 40 [6400/60000 (11%)]\tLoss: 99.659660\n","Train Epoch: 40 [12800/60000 (21%)]\tLoss: 100.633453\n","Train Epoch: 40 [19200/60000 (32%)]\tLoss: 96.927353\n","Train Epoch: 40 [25600/60000 (43%)]\tLoss: 97.213165\n","Train Epoch: 40 [32000/60000 (53%)]\tLoss: 96.755898\n","Train Epoch: 40 [38400/60000 (64%)]\tLoss: 95.642097\n","Train Epoch: 40 [44800/60000 (75%)]\tLoss: 94.682587\n","Train Epoch: 40 [51200/60000 (85%)]\tLoss: 96.898613\n","Train Epoch: 40 [57600/60000 (96%)]\tLoss: 100.923691\n","====> Epoch: 40 Average loss: 96.7079\n","====> Test set loss: 96.8651\n","Train Epoch: 41 [0/60000 (0%)]\tLoss: 98.454628\n","Train Epoch: 41 [6400/60000 (11%)]\tLoss: 97.120422\n","Train Epoch: 41 [12800/60000 (21%)]\tLoss: 96.283661\n","Train Epoch: 41 [19200/60000 (32%)]\tLoss: 95.458740\n","Train Epoch: 41 [25600/60000 (43%)]\tLoss: 96.041420\n","Train Epoch: 41 [32000/60000 (53%)]\tLoss: 97.851151\n","Train Epoch: 41 [38400/60000 (64%)]\tLoss: 96.605171\n","Train Epoch: 41 [44800/60000 (75%)]\tLoss: 93.757431\n","Train Epoch: 41 [51200/60000 (85%)]\tLoss: 98.211823\n","Train Epoch: 41 [57600/60000 (96%)]\tLoss: 93.500961\n","====> Epoch: 41 Average loss: 96.6355\n","====> Test set loss: 96.7226\n","Train Epoch: 42 [0/60000 (0%)]\tLoss: 95.699379\n","Train Epoch: 42 [6400/60000 (11%)]\tLoss: 94.341141\n","Train Epoch: 42 [12800/60000 (21%)]\tLoss: 99.092407\n","Train Epoch: 42 [19200/60000 (32%)]\tLoss: 95.285172\n","Train Epoch: 42 [25600/60000 (43%)]\tLoss: 96.173569\n","Train Epoch: 42 [32000/60000 (53%)]\tLoss: 97.644669\n","Train Epoch: 42 [38400/60000 (64%)]\tLoss: 95.731873\n","Train Epoch: 42 [44800/60000 (75%)]\tLoss: 92.765839\n","Train Epoch: 42 [51200/60000 (85%)]\tLoss: 98.807083\n","Train Epoch: 42 [57600/60000 (96%)]\tLoss: 98.446106\n","====> Epoch: 42 Average loss: 96.5262\n","====> Test set loss: 96.6973\n","Train Epoch: 43 [0/60000 (0%)]\tLoss: 92.099632\n","Train Epoch: 43 [6400/60000 (11%)]\tLoss: 92.839478\n","Train Epoch: 43 [12800/60000 (21%)]\tLoss: 98.811562\n","Train Epoch: 43 [19200/60000 (32%)]\tLoss: 95.857315\n","Train Epoch: 43 [25600/60000 (43%)]\tLoss: 99.988518\n","Train Epoch: 43 [32000/60000 (53%)]\tLoss: 96.978394\n","Train Epoch: 43 [38400/60000 (64%)]\tLoss: 95.506752\n","Train Epoch: 43 [44800/60000 (75%)]\tLoss: 98.384064\n","Train Epoch: 43 [51200/60000 (85%)]\tLoss: 96.430786\n","Train Epoch: 43 [57600/60000 (96%)]\tLoss: 100.918236\n","====> Epoch: 43 Average loss: 96.5364\n","====> Test set loss: 97.2203\n","Train Epoch: 44 [0/60000 (0%)]\tLoss: 93.049706\n","Train Epoch: 44 [6400/60000 (11%)]\tLoss: 93.715446\n","Train Epoch: 44 [12800/60000 (21%)]\tLoss: 98.307404\n","Train Epoch: 44 [19200/60000 (32%)]\tLoss: 96.789246\n","Train Epoch: 44 [25600/60000 (43%)]\tLoss: 94.913345\n","Train Epoch: 44 [32000/60000 (53%)]\tLoss: 96.786987\n","Train Epoch: 44 [38400/60000 (64%)]\tLoss: 96.487442\n","Train Epoch: 44 [44800/60000 (75%)]\tLoss: 94.248543\n","Train Epoch: 44 [51200/60000 (85%)]\tLoss: 94.485970\n","Train Epoch: 44 [57600/60000 (96%)]\tLoss: 93.774887\n","====> Epoch: 44 Average loss: 96.4916\n","====> Test set loss: 96.8852\n","Train Epoch: 45 [0/60000 (0%)]\tLoss: 98.613785\n","Train Epoch: 45 [6400/60000 (11%)]\tLoss: 95.018707\n","Train Epoch: 45 [12800/60000 (21%)]\tLoss: 97.186172\n","Train Epoch: 45 [19200/60000 (32%)]\tLoss: 96.760765\n","Train Epoch: 45 [25600/60000 (43%)]\tLoss: 97.176521\n","Train Epoch: 45 [32000/60000 (53%)]\tLoss: 95.268784\n","Train Epoch: 45 [38400/60000 (64%)]\tLoss: 92.364868\n","Train Epoch: 45 [44800/60000 (75%)]\tLoss: 98.337914\n","Train Epoch: 45 [51200/60000 (85%)]\tLoss: 93.659210\n","Train Epoch: 45 [57600/60000 (96%)]\tLoss: 97.200256\n","====> Epoch: 45 Average loss: 96.4766\n","====> Test set loss: 96.3859\n","Train Epoch: 46 [0/60000 (0%)]\tLoss: 97.183083\n","Train Epoch: 46 [6400/60000 (11%)]\tLoss: 96.118729\n","Train Epoch: 46 [12800/60000 (21%)]\tLoss: 98.405151\n","Train Epoch: 46 [19200/60000 (32%)]\tLoss: 94.554291\n","Train Epoch: 46 [25600/60000 (43%)]\tLoss: 99.640030\n","Train Epoch: 46 [32000/60000 (53%)]\tLoss: 93.664856\n","Train Epoch: 46 [38400/60000 (64%)]\tLoss: 96.356277\n","Train Epoch: 46 [44800/60000 (75%)]\tLoss: 94.234093\n","Train Epoch: 46 [51200/60000 (85%)]\tLoss: 99.393135\n","Train Epoch: 46 [57600/60000 (96%)]\tLoss: 96.437935\n","====> Epoch: 46 Average loss: 96.4849\n","====> Test set loss: 96.5245\n","Train Epoch: 47 [0/60000 (0%)]\tLoss: 94.467804\n","Train Epoch: 47 [6400/60000 (11%)]\tLoss: 98.599106\n","Train Epoch: 47 [12800/60000 (21%)]\tLoss: 99.605446\n","Train Epoch: 47 [19200/60000 (32%)]\tLoss: 98.624344\n","Train Epoch: 47 [25600/60000 (43%)]\tLoss: 97.010498\n","Train Epoch: 47 [32000/60000 (53%)]\tLoss: 98.566406\n","Train Epoch: 47 [38400/60000 (64%)]\tLoss: 97.753540\n","Train Epoch: 47 [44800/60000 (75%)]\tLoss: 94.521881\n","Train Epoch: 47 [51200/60000 (85%)]\tLoss: 93.623604\n","Train Epoch: 47 [57600/60000 (96%)]\tLoss: 93.483223\n","====> Epoch: 47 Average loss: 96.3650\n","====> Test set loss: 96.4850\n","Train Epoch: 48 [0/60000 (0%)]\tLoss: 95.554718\n","Train Epoch: 48 [6400/60000 (11%)]\tLoss: 94.227562\n","Train Epoch: 48 [12800/60000 (21%)]\tLoss: 93.701691\n","Train Epoch: 48 [19200/60000 (32%)]\tLoss: 99.160866\n","Train Epoch: 48 [25600/60000 (43%)]\tLoss: 97.474808\n","Train Epoch: 48 [32000/60000 (53%)]\tLoss: 96.251762\n","Train Epoch: 48 [38400/60000 (64%)]\tLoss: 96.588074\n","Train Epoch: 48 [44800/60000 (75%)]\tLoss: 99.364403\n","Train Epoch: 48 [51200/60000 (85%)]\tLoss: 97.186203\n","Train Epoch: 48 [57600/60000 (96%)]\tLoss: 97.713867\n","====> Epoch: 48 Average loss: 96.3169\n","====> Test set loss: 96.4978\n","Train Epoch: 49 [0/60000 (0%)]\tLoss: 100.357559\n","Train Epoch: 49 [6400/60000 (11%)]\tLoss: 96.476158\n","Train Epoch: 49 [12800/60000 (21%)]\tLoss: 96.736618\n","Train Epoch: 49 [19200/60000 (32%)]\tLoss: 97.100929\n","Train Epoch: 49 [25600/60000 (43%)]\tLoss: 98.083969\n","Train Epoch: 49 [32000/60000 (53%)]\tLoss: 99.371964\n","Train Epoch: 49 [38400/60000 (64%)]\tLoss: 98.331230\n","Train Epoch: 49 [44800/60000 (75%)]\tLoss: 98.650505\n","Train Epoch: 49 [51200/60000 (85%)]\tLoss: 94.562660\n","Train Epoch: 49 [57600/60000 (96%)]\tLoss: 99.663055\n","====> Epoch: 49 Average loss: 96.2768\n","====> Test set loss: 96.2604\n","Train Epoch: 50 [0/60000 (0%)]\tLoss: 94.961334\n","Train Epoch: 50 [6400/60000 (11%)]\tLoss: 99.467880\n","Train Epoch: 50 [12800/60000 (21%)]\tLoss: 98.083694\n","Train Epoch: 50 [19200/60000 (32%)]\tLoss: 94.896858\n","Train Epoch: 50 [25600/60000 (43%)]\tLoss: 99.404839\n","Train Epoch: 50 [32000/60000 (53%)]\tLoss: 94.480804\n","Train Epoch: 50 [38400/60000 (64%)]\tLoss: 94.890694\n","Train Epoch: 50 [44800/60000 (75%)]\tLoss: 93.383881\n","Train Epoch: 50 [51200/60000 (85%)]\tLoss: 97.247383\n","Train Epoch: 50 [57600/60000 (96%)]\tLoss: 96.464394\n","====> Epoch: 50 Average loss: 96.2344\n","====> Test set loss: 96.4604\n","Train Epoch: 51 [0/60000 (0%)]\tLoss: 97.551498\n","Train Epoch: 51 [6400/60000 (11%)]\tLoss: 90.906708\n","Train Epoch: 51 [12800/60000 (21%)]\tLoss: 90.796555\n","Train Epoch: 51 [19200/60000 (32%)]\tLoss: 98.435364\n","Train Epoch: 51 [25600/60000 (43%)]\tLoss: 94.339752\n","Train Epoch: 51 [32000/60000 (53%)]\tLoss: 94.992500\n","Train Epoch: 51 [38400/60000 (64%)]\tLoss: 96.108826\n","Train Epoch: 51 [44800/60000 (75%)]\tLoss: 96.733795\n","Train Epoch: 51 [51200/60000 (85%)]\tLoss: 96.622131\n","Train Epoch: 51 [57600/60000 (96%)]\tLoss: 91.340553\n","====> Epoch: 51 Average loss: 96.2313\n","====> Test set loss: 96.5906\n","Train Epoch: 52 [0/60000 (0%)]\tLoss: 93.225677\n","Train Epoch: 52 [6400/60000 (11%)]\tLoss: 99.254921\n","Train Epoch: 52 [12800/60000 (21%)]\tLoss: 95.982544\n","Train Epoch: 52 [19200/60000 (32%)]\tLoss: 96.267120\n","Train Epoch: 52 [25600/60000 (43%)]\tLoss: 95.971466\n","Train Epoch: 52 [32000/60000 (53%)]\tLoss: 98.812965\n","Train Epoch: 52 [38400/60000 (64%)]\tLoss: 94.878700\n","Train Epoch: 52 [44800/60000 (75%)]\tLoss: 90.857178\n","Train Epoch: 52 [51200/60000 (85%)]\tLoss: 95.003929\n","Train Epoch: 52 [57600/60000 (96%)]\tLoss: 99.775558\n","====> Epoch: 52 Average loss: 96.2418\n","====> Test set loss: 96.3136\n","Train Epoch: 53 [0/60000 (0%)]\tLoss: 98.134186\n","Train Epoch: 53 [6400/60000 (11%)]\tLoss: 96.379745\n","Train Epoch: 53 [12800/60000 (21%)]\tLoss: 97.006111\n","Train Epoch: 53 [19200/60000 (32%)]\tLoss: 96.082458\n","Train Epoch: 53 [25600/60000 (43%)]\tLoss: 96.554405\n","Train Epoch: 53 [32000/60000 (53%)]\tLoss: 93.653572\n","Train Epoch: 53 [38400/60000 (64%)]\tLoss: 93.310608\n","Train Epoch: 53 [44800/60000 (75%)]\tLoss: 95.895584\n","Train Epoch: 53 [51200/60000 (85%)]\tLoss: 98.231773\n","Train Epoch: 53 [57600/60000 (96%)]\tLoss: 95.657829\n","====> Epoch: 53 Average loss: 96.1551\n","====> Test set loss: 96.2312\n","Train Epoch: 54 [0/60000 (0%)]\tLoss: 93.379860\n","Train Epoch: 54 [6400/60000 (11%)]\tLoss: 98.994278\n","Train Epoch: 54 [12800/60000 (21%)]\tLoss: 92.586487\n","Train Epoch: 54 [19200/60000 (32%)]\tLoss: 98.595711\n","Train Epoch: 54 [25600/60000 (43%)]\tLoss: 95.942192\n","Train Epoch: 54 [32000/60000 (53%)]\tLoss: 98.542923\n","Train Epoch: 54 [38400/60000 (64%)]\tLoss: 100.293381\n","Train Epoch: 54 [44800/60000 (75%)]\tLoss: 97.418640\n","Train Epoch: 54 [51200/60000 (85%)]\tLoss: 98.048828\n","Train Epoch: 54 [57600/60000 (96%)]\tLoss: 92.555344\n","====> Epoch: 54 Average loss: 96.1242\n","====> Test set loss: 96.2446\n","Train Epoch: 55 [0/60000 (0%)]\tLoss: 96.134346\n","Train Epoch: 55 [6400/60000 (11%)]\tLoss: 94.882629\n","Train Epoch: 55 [12800/60000 (21%)]\tLoss: 96.370140\n","Train Epoch: 55 [19200/60000 (32%)]\tLoss: 96.717148\n","Train Epoch: 55 [25600/60000 (43%)]\tLoss: 94.020416\n","Train Epoch: 55 [32000/60000 (53%)]\tLoss: 98.610909\n","Train Epoch: 55 [38400/60000 (64%)]\tLoss: 96.130440\n","Train Epoch: 55 [44800/60000 (75%)]\tLoss: 93.738350\n","Train Epoch: 55 [51200/60000 (85%)]\tLoss: 96.566742\n","Train Epoch: 55 [57600/60000 (96%)]\tLoss: 97.230789\n","====> Epoch: 55 Average loss: 96.0593\n","====> Test set loss: 96.2673\n","Train Epoch: 56 [0/60000 (0%)]\tLoss: 96.218185\n","Train Epoch: 56 [6400/60000 (11%)]\tLoss: 95.895790\n","Train Epoch: 56 [12800/60000 (21%)]\tLoss: 95.584793\n","Train Epoch: 56 [19200/60000 (32%)]\tLoss: 97.447212\n","Train Epoch: 56 [25600/60000 (43%)]\tLoss: 96.492249\n","Train Epoch: 56 [32000/60000 (53%)]\tLoss: 97.889900\n","Train Epoch: 56 [38400/60000 (64%)]\tLoss: 95.438141\n","Train Epoch: 56 [44800/60000 (75%)]\tLoss: 94.746033\n","Train Epoch: 56 [51200/60000 (85%)]\tLoss: 93.415421\n","Train Epoch: 56 [57600/60000 (96%)]\tLoss: 99.693100\n","====> Epoch: 56 Average loss: 96.0182\n","====> Test set loss: 96.0385\n","Train Epoch: 57 [0/60000 (0%)]\tLoss: 92.325699\n","Train Epoch: 57 [6400/60000 (11%)]\tLoss: 95.473892\n","Train Epoch: 57 [12800/60000 (21%)]\tLoss: 95.085266\n","Train Epoch: 57 [19200/60000 (32%)]\tLoss: 96.688309\n","Train Epoch: 57 [25600/60000 (43%)]\tLoss: 96.511292\n","Train Epoch: 57 [32000/60000 (53%)]\tLoss: 95.586075\n","Train Epoch: 57 [38400/60000 (64%)]\tLoss: 92.156342\n","Train Epoch: 57 [44800/60000 (75%)]\tLoss: 97.935867\n","Train Epoch: 57 [51200/60000 (85%)]\tLoss: 93.474945\n","Train Epoch: 57 [57600/60000 (96%)]\tLoss: 96.724419\n","====> Epoch: 57 Average loss: 96.0121\n","====> Test set loss: 96.1777\n","Train Epoch: 58 [0/60000 (0%)]\tLoss: 97.015297\n","Train Epoch: 58 [6400/60000 (11%)]\tLoss: 94.993866\n","Train Epoch: 58 [12800/60000 (21%)]\tLoss: 97.822762\n","Train Epoch: 58 [19200/60000 (32%)]\tLoss: 94.612732\n","Train Epoch: 58 [25600/60000 (43%)]\tLoss: 96.764732\n","Train Epoch: 58 [32000/60000 (53%)]\tLoss: 95.345673\n","Train Epoch: 58 [38400/60000 (64%)]\tLoss: 92.988029\n","Train Epoch: 58 [44800/60000 (75%)]\tLoss: 99.948441\n","Train Epoch: 58 [51200/60000 (85%)]\tLoss: 95.323135\n","Train Epoch: 58 [57600/60000 (96%)]\tLoss: 97.557632\n","====> Epoch: 58 Average loss: 95.9741\n","====> Test set loss: 96.1758\n","Train Epoch: 59 [0/60000 (0%)]\tLoss: 94.465714\n","Train Epoch: 59 [6400/60000 (11%)]\tLoss: 99.248177\n","Train Epoch: 59 [12800/60000 (21%)]\tLoss: 95.157654\n","Train Epoch: 59 [19200/60000 (32%)]\tLoss: 93.569839\n","Train Epoch: 59 [25600/60000 (43%)]\tLoss: 96.672028\n","Train Epoch: 59 [32000/60000 (53%)]\tLoss: 95.066292\n","Train Epoch: 59 [38400/60000 (64%)]\tLoss: 94.052429\n","Train Epoch: 59 [44800/60000 (75%)]\tLoss: 95.265976\n","Train Epoch: 59 [51200/60000 (85%)]\tLoss: 98.639343\n","Train Epoch: 59 [57600/60000 (96%)]\tLoss: 94.881645\n","====> Epoch: 59 Average loss: 95.9311\n","====> Test set loss: 96.2646\n","Train Epoch: 60 [0/60000 (0%)]\tLoss: 97.108780\n","Train Epoch: 60 [6400/60000 (11%)]\tLoss: 96.486481\n","Train Epoch: 60 [12800/60000 (21%)]\tLoss: 96.134315\n","Train Epoch: 60 [19200/60000 (32%)]\tLoss: 96.568855\n","Train Epoch: 60 [25600/60000 (43%)]\tLoss: 95.912148\n","Train Epoch: 60 [32000/60000 (53%)]\tLoss: 98.791183\n","Train Epoch: 60 [38400/60000 (64%)]\tLoss: 97.076645\n","Train Epoch: 60 [44800/60000 (75%)]\tLoss: 97.304420\n","Train Epoch: 60 [51200/60000 (85%)]\tLoss: 90.270554\n","Train Epoch: 60 [57600/60000 (96%)]\tLoss: 93.918724\n","====> Epoch: 60 Average loss: 95.9067\n","====> Test set loss: 96.0196\n","Train Epoch: 61 [0/60000 (0%)]\tLoss: 97.740829\n","Train Epoch: 61 [6400/60000 (11%)]\tLoss: 97.994812\n","Train Epoch: 61 [12800/60000 (21%)]\tLoss: 94.294434\n","Train Epoch: 61 [19200/60000 (32%)]\tLoss: 95.796310\n","Train Epoch: 61 [25600/60000 (43%)]\tLoss: 94.907684\n","Train Epoch: 61 [32000/60000 (53%)]\tLoss: 98.997368\n","Train Epoch: 61 [38400/60000 (64%)]\tLoss: 94.961182\n","Train Epoch: 61 [44800/60000 (75%)]\tLoss: 93.710030\n","Train Epoch: 61 [51200/60000 (85%)]\tLoss: 92.849663\n","Train Epoch: 61 [57600/60000 (96%)]\tLoss: 94.907974\n","====> Epoch: 61 Average loss: 95.9268\n","====> Test set loss: 96.3082\n","Train Epoch: 62 [0/60000 (0%)]\tLoss: 91.426422\n","Train Epoch: 62 [6400/60000 (11%)]\tLoss: 95.855255\n","Train Epoch: 62 [12800/60000 (21%)]\tLoss: 96.725616\n","Train Epoch: 62 [19200/60000 (32%)]\tLoss: 100.693314\n","Train Epoch: 62 [25600/60000 (43%)]\tLoss: 98.622787\n","Train Epoch: 62 [32000/60000 (53%)]\tLoss: 96.113113\n","Train Epoch: 62 [38400/60000 (64%)]\tLoss: 98.884766\n","Train Epoch: 62 [44800/60000 (75%)]\tLoss: 91.974266\n","Train Epoch: 62 [51200/60000 (85%)]\tLoss: 98.156540\n","Train Epoch: 62 [57600/60000 (96%)]\tLoss: 91.661774\n","====> Epoch: 62 Average loss: 95.8652\n","====> Test set loss: 96.0489\n","Train Epoch: 63 [0/60000 (0%)]\tLoss: 96.542969\n","Train Epoch: 63 [6400/60000 (11%)]\tLoss: 93.537613\n","Train Epoch: 63 [12800/60000 (21%)]\tLoss: 98.171112\n","Train Epoch: 63 [19200/60000 (32%)]\tLoss: 96.123802\n","Train Epoch: 63 [25600/60000 (43%)]\tLoss: 96.250969\n","Train Epoch: 63 [32000/60000 (53%)]\tLoss: 95.119156\n","Train Epoch: 63 [38400/60000 (64%)]\tLoss: 96.881226\n","Train Epoch: 63 [44800/60000 (75%)]\tLoss: 94.955154\n","Train Epoch: 63 [51200/60000 (85%)]\tLoss: 98.172447\n","Train Epoch: 63 [57600/60000 (96%)]\tLoss: 94.510887\n","====> Epoch: 63 Average loss: 95.8699\n","====> Test set loss: 95.8997\n","Train Epoch: 64 [0/60000 (0%)]\tLoss: 91.876251\n","Train Epoch: 64 [6400/60000 (11%)]\tLoss: 94.428345\n","Train Epoch: 64 [12800/60000 (21%)]\tLoss: 95.178444\n","Train Epoch: 64 [19200/60000 (32%)]\tLoss: 95.652840\n","Train Epoch: 64 [25600/60000 (43%)]\tLoss: 98.787674\n","Train Epoch: 64 [32000/60000 (53%)]\tLoss: 98.945648\n","Train Epoch: 64 [38400/60000 (64%)]\tLoss: 95.940399\n","Train Epoch: 64 [44800/60000 (75%)]\tLoss: 93.679184\n","Train Epoch: 64 [51200/60000 (85%)]\tLoss: 96.717148\n","Train Epoch: 64 [57600/60000 (96%)]\tLoss: 94.151627\n","====> Epoch: 64 Average loss: 95.8420\n","====> Test set loss: 95.9208\n","Train Epoch: 65 [0/60000 (0%)]\tLoss: 97.483833\n","Train Epoch: 65 [6400/60000 (11%)]\tLoss: 93.305801\n","Train Epoch: 65 [12800/60000 (21%)]\tLoss: 92.108124\n","Train Epoch: 65 [19200/60000 (32%)]\tLoss: 94.033371\n","Train Epoch: 65 [25600/60000 (43%)]\tLoss: 97.588326\n","Train Epoch: 65 [32000/60000 (53%)]\tLoss: 94.094391\n","Train Epoch: 65 [38400/60000 (64%)]\tLoss: 97.251099\n","Train Epoch: 65 [44800/60000 (75%)]\tLoss: 93.273972\n","Train Epoch: 65 [51200/60000 (85%)]\tLoss: 94.044128\n","Train Epoch: 65 [57600/60000 (96%)]\tLoss: 94.864365\n","====> Epoch: 65 Average loss: 95.7859\n","====> Test set loss: 95.9083\n","Train Epoch: 66 [0/60000 (0%)]\tLoss: 95.213440\n","Train Epoch: 66 [6400/60000 (11%)]\tLoss: 92.711121\n","Train Epoch: 66 [12800/60000 (21%)]\tLoss: 93.694969\n","Train Epoch: 66 [19200/60000 (32%)]\tLoss: 95.052643\n","Train Epoch: 66 [25600/60000 (43%)]\tLoss: 98.455215\n","Train Epoch: 66 [32000/60000 (53%)]\tLoss: 97.918198\n","Train Epoch: 66 [38400/60000 (64%)]\tLoss: 97.654472\n","Train Epoch: 66 [44800/60000 (75%)]\tLoss: 96.976837\n","Train Epoch: 66 [51200/60000 (85%)]\tLoss: 95.776093\n","Train Epoch: 66 [57600/60000 (96%)]\tLoss: 96.427887\n","====> Epoch: 66 Average loss: 95.8013\n","====> Test set loss: 95.8492\n","Train Epoch: 67 [0/60000 (0%)]\tLoss: 98.152939\n","Train Epoch: 67 [6400/60000 (11%)]\tLoss: 89.308281\n","Train Epoch: 67 [12800/60000 (21%)]\tLoss: 94.535889\n","Train Epoch: 67 [19200/60000 (32%)]\tLoss: 93.990425\n","Train Epoch: 67 [25600/60000 (43%)]\tLoss: 96.182091\n","Train Epoch: 67 [32000/60000 (53%)]\tLoss: 95.099403\n","Train Epoch: 67 [38400/60000 (64%)]\tLoss: 94.833191\n","Train Epoch: 67 [44800/60000 (75%)]\tLoss: 97.876801\n","Train Epoch: 67 [51200/60000 (85%)]\tLoss: 98.522644\n","Train Epoch: 67 [57600/60000 (96%)]\tLoss: 95.691879\n","====> Epoch: 67 Average loss: 95.7859\n","====> Test set loss: 96.0880\n","Train Epoch: 68 [0/60000 (0%)]\tLoss: 98.428329\n","Train Epoch: 68 [6400/60000 (11%)]\tLoss: 97.481026\n","Train Epoch: 68 [12800/60000 (21%)]\tLoss: 92.559906\n","Train Epoch: 68 [19200/60000 (32%)]\tLoss: 99.900536\n","Train Epoch: 68 [25600/60000 (43%)]\tLoss: 96.528854\n","Train Epoch: 68 [32000/60000 (53%)]\tLoss: 95.654716\n","Train Epoch: 68 [38400/60000 (64%)]\tLoss: 94.910538\n","Train Epoch: 68 [44800/60000 (75%)]\tLoss: 96.446777\n","Train Epoch: 68 [51200/60000 (85%)]\tLoss: 94.690445\n","Train Epoch: 68 [57600/60000 (96%)]\tLoss: 95.581955\n","====> Epoch: 68 Average loss: 95.7763\n","====> Test set loss: 95.9571\n","Train Epoch: 69 [0/60000 (0%)]\tLoss: 93.608955\n","Train Epoch: 69 [6400/60000 (11%)]\tLoss: 94.276581\n","Train Epoch: 69 [12800/60000 (21%)]\tLoss: 93.770950\n","Train Epoch: 69 [19200/60000 (32%)]\tLoss: 93.954720\n","Train Epoch: 69 [25600/60000 (43%)]\tLoss: 94.053978\n","Train Epoch: 69 [32000/60000 (53%)]\tLoss: 92.191208\n","Train Epoch: 69 [38400/60000 (64%)]\tLoss: 100.525139\n","Train Epoch: 69 [44800/60000 (75%)]\tLoss: 97.269821\n","Train Epoch: 69 [51200/60000 (85%)]\tLoss: 96.562943\n","Train Epoch: 69 [57600/60000 (96%)]\tLoss: 95.712189\n","====> Epoch: 69 Average loss: 95.6946\n","====> Test set loss: 95.8541\n","Train Epoch: 70 [0/60000 (0%)]\tLoss: 95.971176\n","Train Epoch: 70 [6400/60000 (11%)]\tLoss: 97.010101\n","Train Epoch: 70 [12800/60000 (21%)]\tLoss: 94.506805\n","Train Epoch: 70 [19200/60000 (32%)]\tLoss: 95.387459\n","Train Epoch: 70 [25600/60000 (43%)]\tLoss: 98.536270\n","Train Epoch: 70 [32000/60000 (53%)]\tLoss: 93.791382\n","Train Epoch: 70 [38400/60000 (64%)]\tLoss: 98.982697\n","Train Epoch: 70 [44800/60000 (75%)]\tLoss: 95.242233\n","Train Epoch: 70 [51200/60000 (85%)]\tLoss: 95.082512\n","Train Epoch: 70 [57600/60000 (96%)]\tLoss: 97.612335\n","====> Epoch: 70 Average loss: 95.6921\n","====> Test set loss: 95.8846\n","Train Epoch: 71 [0/60000 (0%)]\tLoss: 99.634598\n","Train Epoch: 71 [6400/60000 (11%)]\tLoss: 95.171677\n","Train Epoch: 71 [12800/60000 (21%)]\tLoss: 95.993958\n","Train Epoch: 71 [19200/60000 (32%)]\tLoss: 92.416946\n","Train Epoch: 71 [25600/60000 (43%)]\tLoss: 95.828323\n","Train Epoch: 71 [32000/60000 (53%)]\tLoss: 98.662979\n","Train Epoch: 71 [38400/60000 (64%)]\tLoss: 96.123474\n","Train Epoch: 71 [44800/60000 (75%)]\tLoss: 98.943413\n","Train Epoch: 71 [51200/60000 (85%)]\tLoss: 95.544205\n","Train Epoch: 71 [57600/60000 (96%)]\tLoss: 92.815071\n","====> Epoch: 71 Average loss: 95.6386\n","====> Test set loss: 95.7374\n","Train Epoch: 72 [0/60000 (0%)]\tLoss: 96.376236\n","Train Epoch: 72 [6400/60000 (11%)]\tLoss: 100.278496\n","Train Epoch: 72 [12800/60000 (21%)]\tLoss: 96.152374\n","Train Epoch: 72 [19200/60000 (32%)]\tLoss: 100.756187\n","Train Epoch: 72 [25600/60000 (43%)]\tLoss: 94.635422\n","Train Epoch: 72 [32000/60000 (53%)]\tLoss: 98.380905\n","Train Epoch: 72 [38400/60000 (64%)]\tLoss: 93.782204\n","Train Epoch: 72 [44800/60000 (75%)]\tLoss: 96.271004\n","Train Epoch: 72 [51200/60000 (85%)]\tLoss: 92.201775\n","Train Epoch: 72 [57600/60000 (96%)]\tLoss: 96.330048\n","====> Epoch: 72 Average loss: 95.6603\n","====> Test set loss: 95.8897\n","Train Epoch: 73 [0/60000 (0%)]\tLoss: 94.802483\n","Train Epoch: 73 [6400/60000 (11%)]\tLoss: 92.557388\n","Train Epoch: 73 [12800/60000 (21%)]\tLoss: 95.574348\n","Train Epoch: 73 [19200/60000 (32%)]\tLoss: 95.002838\n","Train Epoch: 73 [25600/60000 (43%)]\tLoss: 95.441010\n","Train Epoch: 73 [32000/60000 (53%)]\tLoss: 95.446953\n","Train Epoch: 73 [38400/60000 (64%)]\tLoss: 94.474518\n","Train Epoch: 73 [44800/60000 (75%)]\tLoss: 99.263107\n","Train Epoch: 73 [51200/60000 (85%)]\tLoss: 96.005486\n","Train Epoch: 73 [57600/60000 (96%)]\tLoss: 94.998329\n","====> Epoch: 73 Average loss: 95.5830\n","====> Test set loss: 95.9588\n","Train Epoch: 74 [0/60000 (0%)]\tLoss: 95.544212\n","Train Epoch: 74 [6400/60000 (11%)]\tLoss: 97.699257\n","Train Epoch: 74 [12800/60000 (21%)]\tLoss: 92.222466\n","Train Epoch: 74 [19200/60000 (32%)]\tLoss: 93.480118\n","Train Epoch: 74 [25600/60000 (43%)]\tLoss: 91.929474\n","Train Epoch: 74 [32000/60000 (53%)]\tLoss: 99.041794\n","Train Epoch: 74 [38400/60000 (64%)]\tLoss: 96.602081\n","Train Epoch: 74 [44800/60000 (75%)]\tLoss: 98.452759\n","Train Epoch: 74 [51200/60000 (85%)]\tLoss: 96.417480\n","Train Epoch: 74 [57600/60000 (96%)]\tLoss: 96.604294\n","====> Epoch: 74 Average loss: 95.6325\n","====> Test set loss: 95.6497\n","Train Epoch: 75 [0/60000 (0%)]\tLoss: 97.573196\n","Train Epoch: 75 [6400/60000 (11%)]\tLoss: 96.683609\n","Train Epoch: 75 [12800/60000 (21%)]\tLoss: 94.701546\n","Train Epoch: 75 [19200/60000 (32%)]\tLoss: 93.682922\n","Train Epoch: 75 [25600/60000 (43%)]\tLoss: 97.382904\n","Train Epoch: 75 [32000/60000 (53%)]\tLoss: 96.727692\n","Train Epoch: 75 [38400/60000 (64%)]\tLoss: 95.884491\n","Train Epoch: 75 [44800/60000 (75%)]\tLoss: 95.378677\n","Train Epoch: 75 [51200/60000 (85%)]\tLoss: 92.785194\n","Train Epoch: 75 [57600/60000 (96%)]\tLoss: 97.993065\n","====> Epoch: 75 Average loss: 95.5742\n","====> Test set loss: 95.9137\n","Train Epoch: 76 [0/60000 (0%)]\tLoss: 100.903122\n","Train Epoch: 76 [6400/60000 (11%)]\tLoss: 93.406342\n","Train Epoch: 76 [12800/60000 (21%)]\tLoss: 93.312477\n","Train Epoch: 76 [19200/60000 (32%)]\tLoss: 92.376984\n","Train Epoch: 76 [25600/60000 (43%)]\tLoss: 92.039749\n","Train Epoch: 76 [32000/60000 (53%)]\tLoss: 93.203041\n","Train Epoch: 76 [38400/60000 (64%)]\tLoss: 94.856613\n","Train Epoch: 76 [44800/60000 (75%)]\tLoss: 93.143837\n","Train Epoch: 76 [51200/60000 (85%)]\tLoss: 95.151611\n","Train Epoch: 76 [57600/60000 (96%)]\tLoss: 97.482368\n","====> Epoch: 76 Average loss: 95.5344\n","====> Test set loss: 95.9963\n","Train Epoch: 77 [0/60000 (0%)]\tLoss: 99.594727\n","Train Epoch: 77 [6400/60000 (11%)]\tLoss: 96.156822\n","Train Epoch: 77 [12800/60000 (21%)]\tLoss: 93.883850\n","Train Epoch: 77 [19200/60000 (32%)]\tLoss: 92.852127\n","Train Epoch: 77 [25600/60000 (43%)]\tLoss: 92.353928\n","Train Epoch: 77 [32000/60000 (53%)]\tLoss: 91.171585\n","Train Epoch: 77 [38400/60000 (64%)]\tLoss: 98.943100\n","Train Epoch: 77 [44800/60000 (75%)]\tLoss: 93.737366\n","Train Epoch: 77 [51200/60000 (85%)]\tLoss: 92.670547\n","Train Epoch: 77 [57600/60000 (96%)]\tLoss: 92.719208\n","====> Epoch: 77 Average loss: 95.5536\n","====> Test set loss: 95.5334\n","Train Epoch: 78 [0/60000 (0%)]\tLoss: 93.887650\n","Train Epoch: 78 [6400/60000 (11%)]\tLoss: 92.901001\n","Train Epoch: 78 [12800/60000 (21%)]\tLoss: 93.818863\n","Train Epoch: 78 [19200/60000 (32%)]\tLoss: 92.205368\n","Train Epoch: 78 [25600/60000 (43%)]\tLoss: 92.770935\n","Train Epoch: 78 [32000/60000 (53%)]\tLoss: 93.331223\n","Train Epoch: 78 [38400/60000 (64%)]\tLoss: 93.683769\n","Train Epoch: 78 [44800/60000 (75%)]\tLoss: 93.001694\n","Train Epoch: 78 [51200/60000 (85%)]\tLoss: 95.187553\n","Train Epoch: 78 [57600/60000 (96%)]\tLoss: 96.652435\n","====> Epoch: 78 Average loss: 95.4656\n","====> Test set loss: 95.5913\n","Train Epoch: 79 [0/60000 (0%)]\tLoss: 94.195343\n","Train Epoch: 79 [6400/60000 (11%)]\tLoss: 93.504654\n","Train Epoch: 79 [12800/60000 (21%)]\tLoss: 97.579788\n","Train Epoch: 79 [19200/60000 (32%)]\tLoss: 97.937805\n","Train Epoch: 79 [25600/60000 (43%)]\tLoss: 92.932655\n","Train Epoch: 79 [32000/60000 (53%)]\tLoss: 94.874962\n","Train Epoch: 79 [38400/60000 (64%)]\tLoss: 94.778061\n","Train Epoch: 79 [44800/60000 (75%)]\tLoss: 99.382652\n","Train Epoch: 79 [51200/60000 (85%)]\tLoss: 92.375534\n","Train Epoch: 79 [57600/60000 (96%)]\tLoss: 96.374947\n","====> Epoch: 79 Average loss: 95.5133\n","====> Test set loss: 95.6748\n","Train Epoch: 80 [0/60000 (0%)]\tLoss: 92.313477\n","Train Epoch: 80 [6400/60000 (11%)]\tLoss: 97.721863\n","Train Epoch: 80 [12800/60000 (21%)]\tLoss: 93.343391\n","Train Epoch: 80 [19200/60000 (32%)]\tLoss: 94.115845\n","Train Epoch: 80 [25600/60000 (43%)]\tLoss: 92.107330\n","Train Epoch: 80 [32000/60000 (53%)]\tLoss: 96.309265\n","Train Epoch: 80 [38400/60000 (64%)]\tLoss: 93.284271\n","Train Epoch: 80 [44800/60000 (75%)]\tLoss: 99.844223\n","Train Epoch: 80 [51200/60000 (85%)]\tLoss: 95.055252\n","Train Epoch: 80 [57600/60000 (96%)]\tLoss: 95.732635\n","====> Epoch: 80 Average loss: 95.4589\n","====> Test set loss: 95.4556\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"28fhdlhZlafY","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1619552137460,"user_tz":240,"elapsed":626,"user":{"displayName":"Gabriel Laberge","photoUrl":"","userId":"01460729194346842434"}},"outputId":"1d6a85cb-7efe-45f9-cfb0-0f56e7cca43d"},"source":["plot_learning_curve(logger.losses_train, logger.losses_test)\n","plt.savefig(f\"{root}/results/VAE/{model.name}_loss.pdf\", bbox_inches='tight')"],"execution_count":92,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"0TFBpGer1Dy5"},"source":[""],"execution_count":null,"outputs":[]}]}